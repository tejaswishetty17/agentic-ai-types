{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejaswishetty17/Agentic-AI/blob/main/Fine_Tuning_a_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyUzG39FToMs"
      },
      "source": [
        "# \ud83d\udd27 Fine-Tuning DistilBERT for Sentiment Classification using PEFT and LoRA\n",
        "\n",
        "In this tutorial, we will fine-tune a lightweight transformer model \u2014 `distilbert/distilbert-base-uncased` \u2014 for a binary sentiment classification task. Given a movie review, our model will predict whether the sentiment is **positive (1)** or **negative (0)**.\n",
        "\n",
        "We'll use the **IMDB movie review dataset** from Hugging Face (`stanfordnlp/imdb`) and apply **Parameter-Efficient Fine-Tuning (PEFT)** techniques, specifically **LoRA (Low-Rank Adaptation)**, to reduce the number of trainable parameters and make the training more efficient, especially in resource-constrained environments like Google Colab.\n",
        "\n",
        "By the end of this tutorial, you'll have a fine-tuned DistilBERT model capable of classifying movie reviews with high accuracy \u2014 using just a fraction of the original training cost.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82TtM9WSI4wG",
        "outputId": "48eaf347-b210-4a11-8986-3c53be5e682d"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes peft datasets evaluate fsspec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6qbRAqcTUtgH"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict, Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
        "import evaluate\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJ8DaYYkVxfX",
        "outputId": "6aec6c89-55a3-48e0-f563-17d4d3b69a24"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "\n",
        "id2label = {0: \"Negative\", 1:\"Positive\"}\n",
        "label2id = {\"Negative\": 0, \"Positive\": 1}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels = 2, id2label=id2label, label2id=label2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ampiQHNSWbtW"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKzl7TR-YzcU"
      },
      "source": [
        "Lets check how the base model performs for classification task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qopvIqFaXQdN",
        "outputId": "58e5ba82-9935-4c42-a807-6704d7b36982"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.1667, -0.1363]], grad_fn=<AddmmBackward0>)\n",
            "This movie was an absolute masterpiece. The storytelling and acting were top-notch! - Positive\n",
            "tensor([[-0.1818, -0.1419]], grad_fn=<AddmmBackward0>)\n",
            "I wasted two hours of my life. Poor plot, bad acting, and even worse dialogue. - Positive\n",
            "tensor([[-0.1725, -0.1233]], grad_fn=<AddmmBackward0>)\n",
            "A visually stunning film with a powerful emotional core. I was completely immersed. - Positive\n",
            "tensor([[-0.1750, -0.1248]], grad_fn=<AddmmBackward0>)\n",
            "The pacing was painfully slow and the plot went nowhere. Very disappointing. - Positive\n",
            "tensor([[-0.1927, -0.1135]], grad_fn=<AddmmBackward0>)\n",
            "Fantastic performances and a beautiful soundtrack. Highly recommend! - Positive\n",
            "tensor([[-0.1732, -0.1285]], grad_fn=<AddmmBackward0>)\n",
            "One of the worst movies I\u2019ve seen this year. Not even worth watching for free. - Positive\n",
            "tensor([[-0.1585, -0.1183]], grad_fn=<AddmmBackward0>)\n",
            "An emotional rollercoaster. It made me laugh, cry, and everything in between. - Positive\n",
            "tensor([[-0.1500, -0.1288]], grad_fn=<AddmmBackward0>)\n",
            "Predictable and clich\u00e9. Felt like I had seen this exact movie ten times before. - Positive\n",
            "tensor([[-0.2043, -0.0897]], grad_fn=<AddmmBackward0>)\n",
            "Brilliant direction and an unforgettable story. This one stays with you. - Positive\n",
            "tensor([[-0.1700, -0.1806]], grad_fn=<AddmmBackward0>)\n",
            "Terrible editing and a lack of character development ruined what could have been decent. - Negative\n"
          ]
        }
      ],
      "source": [
        "movie_reviews = [\n",
        "    \"This movie was an absolute masterpiece. The storytelling and acting were top-notch!\",\n",
        "    \"I wasted two hours of my life. Poor plot, bad acting, and even worse dialogue.\",\n",
        "    \"A visually stunning film with a powerful emotional core. I was completely immersed.\",\n",
        "    \"The pacing was painfully slow and the plot went nowhere. Very disappointing.\",\n",
        "    \"Fantastic performances and a beautiful soundtrack. Highly recommend!\",\n",
        "    \"One of the worst movies I\u2019ve seen this year. Not even worth watching for free.\",\n",
        "    \"An emotional rollercoaster. It made me laugh, cry, and everything in between.\",\n",
        "    \"Predictable and clich\u00e9. Felt like I had seen this exact movie ten times before.\",\n",
        "    \"Brilliant direction and an unforgettable story. This one stays with you.\",\n",
        "    \"Terrible editing and a lack of character development ruined what could have been decent.\"\n",
        "]\n",
        "\n",
        "for text in movie_reviews:\n",
        "  inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "  logits = model(inputs).logits\n",
        "  print(logits)\n",
        "  predictions = torch.argmax(logits)\n",
        "  print(text + \" - \" + id2label[predictions.tolist()])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyr5M48s6iR-",
        "outputId": "eaafbed3-8a16-4082-c0a3-7b6cdc8c77de"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VYKEMRoZEJF"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_B0-15EzZHzd",
        "outputId": "dfd7494a-da48-4824-f770-2e24400058c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': 'Darcy and her young daughter Pamela are heading out to the country where her mum\\'s boyfriend Peter left his doctor\\'s position in the city to become a writer and fix up a bed and breakfast inn. Although this inn has a terrible past and Pamela learns from one the girl\\'s who lives in the town that a deformed witch once reside in that house. They called her the \\'Tooth Fairy\\' as she would kill kids after getting their last baby tooth. This work on the inn, has awoken the \\'Tooth Fairy\\'. Now she has her sights on Pamela and her last baby tooth, but if any gets in the way they face the same fate that awaits Pamela.<br /><br />This flick\\'s old folk myth of the \\'Tooth Fairy\\' doesn\\'t paint her in a very generous way, as you would believe when you were a child. Don\\'t they just love turning happy childhood memories into nightmares! Another one which did fall into the same category was \"Darkness Falls (2003)\". I can\\'t compare how similar they are in the premises, because I haven\\'t seen the latter, but I mostly read they have basically share the same idea. For a little straight to DVD film, this DTV effort looks good and has some promising images surrounding the senseless and traditionally by the book plot device. Low expectations are needed, as I wouldn\\'t class it as an success, but I found it be to marginally entertaining.<br /><br />Cory Strode and Cookie Rae Brown\\'s story or background for this \\'Tooth Fairy\\' character is completely bare with it leaning more towards a slasher vehicle than anything really supernatural. Silly is a good way to describe what\\'s happening in this poorly scripted story, but it never really feels like a fairytale horror. The dialogues can seem rather redundant and morally hounded. While the acting is simply sub-par with the bland characters they have to work off, but director Chuck Bowman offers up some inventive blood splatter and terribly nasty jolts. This kinda makes up for the lack of suspense, the zero scares and generic tone. His direction is reasonably earnest and visually able, where he gets some atmospheric lighting contrasting well with its slick photography. The promising opening scene is creepily effective. His pacing can slow up in parts and there\\'s the odd and unnecessary slow-motion scene put in, but nonetheless it never gets too stodgy with something active occurring which made sure that I wasn\\'t bored.<br /><br />The make-up special effects provided the goods, as there\\'s enough repulsive gruel and the Tooth Fairy\\'s appearance is especially gooey. The figure of the tooth Fairy can look threatening in its black robe, bubbling make-up and swift movements. Being on location helps carve out a more natural feel and can get atmospherically rich in its sense of eeriness. Child actors can be incredibly annoying, but Nicole Mu\u00f1oz was decent in her part. Lochlyn Munro and Chandra West are somewhat solid, but can be a little too causal in their performances as Peter and Darcey. The radiantly gorgeous Carrie Anne Fleming is one of their lodgers. P.J Soles shows up in small part as a superstitious neighbour who tries to warn them about the evil that lurks at the inn.<br /><br />I thought it was a okay time-waster that has a sound concept, which just isn\\'t fleshed out enough and the execution is pretty textbook stuff. Watchable nonsense, but at the same time extremely forgettable.',\n",
              " 'label': 0}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['train'][2000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GxgtbABcCtz",
        "outputId": "35ae3fb3-ef57-414e-bb72-31c4ba1bb543"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 800\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 800\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "train_small = dataset[\"train\"].shuffle(seed=42).select(range(800))\n",
        "test_small = dataset[\"test\"].shuffle(seed=42).select(range(800))\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    \"train\":train_small,\n",
        "    \"test\":test_small\n",
        "})\n",
        "\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWTUuFqqcoeQ",
        "outputId": "db625084-c40e-476c-e11c-d04593dc84e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': 'George P. Cosmatos\\' \"Rambo: First Blood Part II\" is pure wish-fulfillment. The United States clearly didn\\'t win the war in Vietnam. They caused damage to this country beyond the imaginable and this movie continues the fairy story of the oh-so innocent soldiers. The only bad guys were the leaders of the nation, who made this war happen. The character of Rambo is perfect to notice this. He is extremely patriotic, bemoans that US-Americans didn\\'t appreciate and celebrate the achievements of the single soldier, but has nothing but distrust for leading officers and politicians. Like every film that defends the war (e.g. \"We Were Soldiers\") also this one avoids the need to give a comprehensible reason for the engagement in South Asia. And for that matter also the reason for every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole nation. It would have been better to work on how to deal with the memories, rather than suppressing them. \"Do we get to win this time?\" Yes, you do.',\n",
              " 'label': 0}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"train\"][2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTI0nsGTcuul",
        "outputId": "0d2d2f3a-c251-4f43-da7e-a144215cdfe0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.49)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.array(dataset['train']['label']).sum()/len(dataset['train']['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RqQMI9fNdIcz"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwN8i9FodSN2"
      },
      "outputs": [],
      "source": [
        "#create tokenize function\n",
        "def tokenize_function(examples):\n",
        "  #extact text\n",
        "  text = examples[\"text\"]\n",
        "\n",
        "  #tokenize and truncate text\n",
        "  tokenizer.truncation_side = \"left\"\n",
        "  tokenized_inputs = tokenizer(\n",
        "      text,\n",
        "      return_tensors = \"np\",\n",
        "      truncation = True,\n",
        "      max_length=512\n",
        "  )\n",
        "\n",
        "  return tokenized_inputs\n",
        "\n",
        "#add pad token if none exists\n",
        "if tokenizer.pad_token is None:\n",
        "  tokenizer.add_special_tokens({'pad_token':'[PAD]'})\n",
        "  model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "#tokenize training and validation datasets\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ai_1Up_gexYX"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9U7IJZbOfBMR"
      },
      "outputs": [],
      "source": [
        "# Create a dynamic data collator to pad sequences in each batch to the longest length\n",
        "# Helps ensure efficient batching without padding the entire dataset to max length\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "192e93f9001144e8bc2b3e3e74e0ef85",
            "c0c0ab25effc43608f484fb5da6d18b2",
            "a8a8c5e4775f427e99a310a270ff649d",
            "94c84f25991a4096b53d3b4d584f62cc",
            "e9586ee979bf4f4ebd79e0e48a6d1d3a",
            "6f232ffa23de40978477736c163d63f1",
            "60155069518c42e5871c493281236915",
            "e627e73edab548feae2bd8d50b702ed2",
            "c46351bf2c584fabb77e6d9a684b6e34",
            "fbeb8ee269844ab49a230e0e6b11de6e",
            "d95c5e32427c4af3b6ac95c13be2a88c"
          ]
        },
        "id": "ObrPZcDR69eR",
        "outputId": "a0e03df2-0efa-40c6-d3e5-573c04b5e24f"
      },
      "outputs": [],
      "source": [
        "#import accuracy evaluation metric\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "# Define a metric function for the Trainer to evaluate model accuracy\n",
        "# Applies argmax to predictions and compares with true labels\n",
        "\n",
        "def compute_metrics(p):\n",
        "  predictions, labels = p\n",
        "  predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "  return {\"accuracy\": accuracy.compute(predictions = predictions, references= labels)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "bjJpF4jYLkA-"
      },
      "outputs": [],
      "source": [
        "# Define LoRA config for parameter-efficient fine-tuning on a sequence classification task\n",
        "# Applies LoRA to the 'q_lin' (query linear) layers with low-rank adaptation\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    task_type = \"SEQ_CLS\", # Indicates it's a sequence classification task.\n",
        "    r = 4, #The rank of the LoRA decomposition (lower rank = fewer trainable params).\n",
        "    lora_alpha = 32, #Scaling factor that controls the learning rate within LoRA layers.\n",
        "    lora_dropout = 0.05, #Dropout applied to the LoRA layers to prevent overfitting.\n",
        "    target_modules = ['q_lin'] #Specifies which layers in the model to apply LoRA to (e.g., the query linear layer in attention).\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXrPXP2xL6dP",
        "outputId": "cbb51172-65e2-4044-cfa5-7576f97a5694"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 628,994 || all params: 67,584,004 || trainable%: 0.9307\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "# Apply LoRA configuration to the base model for efficient fine-tuning\n",
        "# Print only trainable parameters\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "I1W8_FgzOCoi"
      },
      "outputs": [],
      "source": [
        "#hyper parameters\n",
        "lr = 1e-3 #size of optimization step #.001\n",
        "batch_size = 4 #number of examples processed per optimziation ste\n",
        "num_epochs = 5 #number of times model runs through training data\n",
        "\n",
        "#define training arguments\n",
        "# define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir= \"distilbert-finetuned2\",\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=num_epochs,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    # Fix 1: Disable Weights & Biases (wandb)\n",
        "    report_to=\"none\",\n",
        "    # Fix 2: Explicitly provide label name\n",
        "    #label_names=[\"labels\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "mi9ISRuk1Fnr",
        "outputId": "aecc9a73-2dbb-4c25-8e7a-07d244f4bc0a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 02:47, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.324701</td>\n",
              "      <td>{'accuracy': 0.87125}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.489694</td>\n",
              "      <td>{'accuracy': 0.86125}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.371400</td>\n",
              "      <td>0.700208</td>\n",
              "      <td>{'accuracy': 0.85875}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.371400</td>\n",
              "      <td>0.673064</td>\n",
              "      <td>{'accuracy': 0.87}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.110000</td>\n",
              "      <td>0.702759</td>\n",
              "      <td>{'accuracy': 0.865}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1000, training_loss=0.24073773193359374, metrics={'train_runtime': 168.3579, 'train_samples_per_second': 23.759, 'train_steps_per_second': 5.94, 'total_flos': 442015308850944.0, 'train_loss': 0.24073773193359374, 'epoch': 5.0})"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# creater trainer object\n",
        "trainer = Trainer(\n",
        "    model=model, # our peft model\n",
        "    args=training_args, # hyperparameters\n",
        "    train_dataset=tokenized_dataset[\"train\"], # training data\n",
        "    eval_dataset=tokenized_dataset[\"test\"], # validation data\n",
        "    tokenizer=tokenizer, # define tokenizer\n",
        "    data_collator=data_collator, # this will dynamically pad examples in each batch to be equal length\n",
        "    compute_metrics=compute_metrics, # evaluates model using compute_metrics() function from before\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fw6qOdIM81MP",
        "outputId": "9d3ddf5a-d534-4077-af7a-26c56b299d4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "This movie was an absolute masterpiece. The storytelling and acting were top-notch! - Positive\n",
            "I wasted two hours of my life. Poor plot, bad acting, and even worse dialogue. - Negative\n",
            "A visually stunning film with a powerful emotional core. I was completely immersed. - Positive\n",
            "The pacing was painfully slow and the plot went nowhere. Very disappointing. - Negative\n",
            "Fantastic performances and a beautiful soundtrack. Highly recommend! - Positive\n",
            "One of the worst movies I\u2019ve seen this year. Not even worth watching for free. - Negative\n",
            "An emotional rollercoaster. It made me laugh, cry, and everything in between. - Positive\n",
            "Predictable and clich\u00e9. Felt like I had seen this exact movie ten times before. - Negative\n",
            "Brilliant direction and an unforgettable story. This one stays with you. - Positive\n",
            "Terrible editing and a lack of character development ruined what could have been decent. - Negative\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "print(device)\n",
        "\n",
        "\n",
        "movie_reviews = [\n",
        "    \"This movie was an absolute masterpiece. The storytelling and acting were top-notch!\",\n",
        "    \"I wasted two hours of my life. Poor plot, bad acting, and even worse dialogue.\",\n",
        "    \"A visually stunning film with a powerful emotional core. I was completely immersed.\",\n",
        "    \"The pacing was painfully slow and the plot went nowhere. Very disappointing.\",\n",
        "    \"Fantastic performances and a beautiful soundtrack. Highly recommend!\",\n",
        "    \"One of the worst movies I\u2019ve seen this year. Not even worth watching for free.\",\n",
        "    \"An emotional rollercoaster. It made me laugh, cry, and everything in between.\",\n",
        "    \"Predictable and clich\u00e9. Felt like I had seen this exact movie ten times before.\",\n",
        "    \"Brilliant direction and an unforgettable story. This one stays with you.\",\n",
        "    \"Terrible editing and a lack of character development ruined what could have been decent.\"\n",
        "]\n",
        "\n",
        "for text in movie_reviews:\n",
        "  inputs = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
        "  logits = model(inputs).logits\n",
        "  predictions = torch.max(logits,1).indices\n",
        "  print(text + \" - \" + id2label[predictions.tolist()[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2ta3HOSo-PXR"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"myfinetuned_model1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV3Zdw5Z-0Ws"
      },
      "source": [
        " Zip the saved model folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8iW0W6f4-xK3",
        "outputId": "1e00e55c-f431-4216-80da-41b194277abc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/myfinetuned_model1.zip'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "shutil.make_archive(\"myfinetuned_model1\", 'zip', \"myfinetuned_model1\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVho9_2y-6pf"
      },
      "source": [
        "Download the zipped file to your local machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "3-2-dyMV-3v0",
        "outputId": "ebe182c8-8b07-4921-ff3d-ab765c57959e"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_a5477e13-c816-4c6a-a49b-0b6f55b0f763\", \"myfinetuned_model1.zip\", 2339221)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(\"/content/myfinetuned_model1.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m49Bfaxl_B7V",
        "outputId": "70cac6a2-0054-499a-dd1b-81ed891f60a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "\n",
        "id2label = {0: \"Negative\", 1: \"Positive\"}\n",
        "label2id = {\"Negative\": 0, \"Positive\": 1}\n",
        "\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "wpKticcD_W5_"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "model = PeftModel.from_pretrained(base_model, \"myfinetuned_model1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxMOLUzW_ZAp",
        "outputId": "c7c470b6-72f1-46b1-c20a-fe24e96a9c76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device used is --- cuda\n",
            "This movie was an absolute masterpiece. The storytelling and acting were top-notch! - Positive\n",
            "I wasted two hours of my life. Poor plot, bad acting, and even worse dialogue. - Negative\n",
            "A visually stunning film with a powerful emotional core. I was completely immersed. - Positive\n",
            "The pacing was painfully slow and the plot went nowhere. Very disappointing. - Negative\n",
            "Fantastic performances and a beautiful soundtrack. Highly recommend! - Positive\n",
            "One of the worst movies I\u2019ve seen this year. Not even worth watching for free. - Negative\n",
            "An emotional rollercoaster. It made me laugh, cry, and everything in between. - Positive\n",
            "Predictable and clich\u00e9. Felt like I had seen this exact movie ten times before. - Negative\n",
            "Brilliant direction and an unforgettable story. This one stays with you. - Positive\n",
            "Terrible editing and a lack of character development ruined what could have been decent. - Negative\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "print(\"Device used is ---\", device)\n",
        "\n",
        "\n",
        "movie_reviews = [\n",
        "    \"This movie was an absolute masterpiece. The storytelling and acting were top-notch!\",\n",
        "    \"I wasted two hours of my life. Poor plot, bad acting, and even worse dialogue.\",\n",
        "    \"A visually stunning film with a powerful emotional core. I was completely immersed.\",\n",
        "    \"The pacing was painfully slow and the plot went nowhere. Very disappointing.\",\n",
        "    \"Fantastic performances and a beautiful soundtrack. Highly recommend!\",\n",
        "    \"One of the worst movies I\u2019ve seen this year. Not even worth watching for free.\",\n",
        "    \"An emotional rollercoaster. It made me laugh, cry, and everything in between.\",\n",
        "    \"Predictable and clich\u00e9. Felt like I had seen this exact movie ten times before.\",\n",
        "    \"Brilliant direction and an unforgettable story. This one stays with you.\",\n",
        "    \"Terrible editing and a lack of character development ruined what could have been decent.\"\n",
        "]\n",
        "\n",
        "# for text in movie_reviews:\n",
        "#   inputs = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
        "#   logits = finetuned_model(inputs).logits\n",
        "#   predictions = torch.argmax(logits)\n",
        "#   print(text + \" - \" + id2label[predictions.tolist()])\n",
        "\n",
        "\n",
        "for text in movie_reviews:\n",
        "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(device) # moving to mps for Mac (can alternatively do 'cpu')\n",
        "\n",
        "    logits = model(inputs).logits\n",
        "    predictions = torch.max(logits,1).indices\n",
        "\n",
        "    print(text + \" - \" + id2label[predictions.tolist()[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ilAUxzki7l_i"
      },
      "outputs": [],
      "source": [
        "#Training the model again after required steps.\n",
        "\n",
        "#hyper parameters\n",
        "lr = 1e-3 #size of optimization step #.001\n",
        "batch_size = 4 #number of examples processed per optimziation ste\n",
        "num_epochs = 5 #number of times model runs through training data\n",
        "\n",
        "#define training arguments\n",
        "# define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir= \"distilbert-finetuned2\",\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=num_epochs,\n",
        "    weight_decay=0.02,\n",
        "    #eval_strategy=\"epoch\",\n",
        "    #save_strategy=\"epoch\",\n",
        "\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=25,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    eval_steps=25,\n",
        "    save_steps=25,\n",
        "    load_best_model_at_end=True,\n",
        "    # Fix 1: Disable Weights & Biases (wandb)\n",
        "    report_to=\"none\",\n",
        "    # Fix 2: Explicitly provide label name\n",
        "    #label_names=[\"labels\"]\n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F2LbvtkE_gKV",
        "outputId": "be2bab18-8637-4017-99e2-a30c39a6f57c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 07:48, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.343400</td>\n",
              "      <td>0.429594</td>\n",
              "      <td>{'accuracy': 0.8675}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.271200</td>\n",
              "      <td>0.415820</td>\n",
              "      <td>{'accuracy': 0.87125}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.236400</td>\n",
              "      <td>0.606903</td>\n",
              "      <td>{'accuracy': 0.83625}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.150900</td>\n",
              "      <td>0.505578</td>\n",
              "      <td>{'accuracy': 0.87}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.057500</td>\n",
              "      <td>0.619315</td>\n",
              "      <td>{'accuracy': 0.855}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.172300</td>\n",
              "      <td>0.601395</td>\n",
              "      <td>{'accuracy': 0.87125}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.377100</td>\n",
              "      <td>0.996105</td>\n",
              "      <td>{'accuracy': 0.80875}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.422500</td>\n",
              "      <td>0.751284</td>\n",
              "      <td>{'accuracy': 0.83875}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.313500</td>\n",
              "      <td>0.587589</td>\n",
              "      <td>{'accuracy': 0.84375}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.264500</td>\n",
              "      <td>0.577157</td>\n",
              "      <td>{'accuracy': 0.84875}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.258100</td>\n",
              "      <td>0.510087</td>\n",
              "      <td>{'accuracy': 0.8725}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.331400</td>\n",
              "      <td>0.476889</td>\n",
              "      <td>{'accuracy': 0.87375}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>0.311600</td>\n",
              "      <td>0.422121</td>\n",
              "      <td>{'accuracy': 0.8725}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.069800</td>\n",
              "      <td>0.463936</td>\n",
              "      <td>{'accuracy': 0.86625}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>0.416500</td>\n",
              "      <td>0.430561</td>\n",
              "      <td>{'accuracy': 0.87625}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.259300</td>\n",
              "      <td>0.421891</td>\n",
              "      <td>{'accuracy': 0.86125}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>425</td>\n",
              "      <td>0.306200</td>\n",
              "      <td>0.456919</td>\n",
              "      <td>{'accuracy': 0.855}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.178900</td>\n",
              "      <td>0.443639</td>\n",
              "      <td>{'accuracy': 0.86625}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>475</td>\n",
              "      <td>0.266200</td>\n",
              "      <td>0.463332</td>\n",
              "      <td>{'accuracy': 0.8625}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.294100</td>\n",
              "      <td>0.425045</td>\n",
              "      <td>{'accuracy': 0.86375}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>525</td>\n",
              "      <td>0.208800</td>\n",
              "      <td>0.435303</td>\n",
              "      <td>{'accuracy': 0.87}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.140800</td>\n",
              "      <td>0.459105</td>\n",
              "      <td>{'accuracy': 0.86125}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>575</td>\n",
              "      <td>0.191200</td>\n",
              "      <td>0.468329</td>\n",
              "      <td>{'accuracy': 0.8725}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.296800</td>\n",
              "      <td>0.484934</td>\n",
              "      <td>{'accuracy': 0.85125}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>625</td>\n",
              "      <td>0.290200</td>\n",
              "      <td>0.418307</td>\n",
              "      <td>{'accuracy': 0.8625}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.222200</td>\n",
              "      <td>0.461505</td>\n",
              "      <td>{'accuracy': 0.8575}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>675</td>\n",
              "      <td>0.246800</td>\n",
              "      <td>0.451194</td>\n",
              "      <td>{'accuracy': 0.8625}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.175100</td>\n",
              "      <td>0.453610</td>\n",
              "      <td>{'accuracy': 0.86875}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>725</td>\n",
              "      <td>0.222000</td>\n",
              "      <td>0.466418</td>\n",
              "      <td>{'accuracy': 0.8625}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.483871</td>\n",
              "      <td>{'accuracy': 0.86125}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>775</td>\n",
              "      <td>0.372600</td>\n",
              "      <td>0.482598</td>\n",
              "      <td>{'accuracy': 0.855}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.291300</td>\n",
              "      <td>0.460826</td>\n",
              "      <td>{'accuracy': 0.87375}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>825</td>\n",
              "      <td>0.224900</td>\n",
              "      <td>0.450975</td>\n",
              "      <td>{'accuracy': 0.87125}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.251700</td>\n",
              "      <td>0.462151</td>\n",
              "      <td>{'accuracy': 0.8575}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>875</td>\n",
              "      <td>0.207200</td>\n",
              "      <td>0.491487</td>\n",
              "      <td>{'accuracy': 0.85875}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.259500</td>\n",
              "      <td>0.487015</td>\n",
              "      <td>{'accuracy': 0.8575}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>925</td>\n",
              "      <td>0.175500</td>\n",
              "      <td>0.465696</td>\n",
              "      <td>{'accuracy': 0.85875}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.075800</td>\n",
              "      <td>0.454747</td>\n",
              "      <td>{'accuracy': 0.86}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>975</td>\n",
              "      <td>0.212100</td>\n",
              "      <td>0.455784</td>\n",
              "      <td>{'accuracy': 0.86}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.233500</td>\n",
              "      <td>0.455091</td>\n",
              "      <td>{'accuracy': 0.86}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1000, training_loss=0.24248940098285676, metrics={'train_runtime': 468.3785, 'train_samples_per_second': 8.54, 'train_steps_per_second': 2.135, 'total_flos': 442015308850944.0, 'train_loss': 0.24248940098285676, 'epoch': 5.0})"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# creater trainer object\n",
        "trainer = Trainer(\n",
        "    model=model, # our peft model\n",
        "    args=training_args, # hyperparameters\n",
        "    train_dataset=tokenized_dataset[\"train\"], # training data\n",
        "    eval_dataset=tokenized_dataset[\"test\"], # validation data\n",
        "    tokenizer=tokenizer, # define tokenizer\n",
        "    data_collator=data_collator, # this will dynamically pad examples in each batch to be equal length\n",
        "    compute_metrics=compute_metrics, # evaluates model using compute_metrics() function from before\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoHhv0AQB9Nx"
      },
      "source": [
        "During training, we noticed that the validation loss increases while the training loss decreases.\n",
        "This is a clear sign of overfitting, where the model memorizes training examples but fails to generalize to new, unseen data.\n",
        "\n",
        "\u2705 Steps to Mitigate Overfitting (Hyperparameter Tuning)\n",
        "To reduce overfitting, consider the following adjustments:\n",
        "\n",
        "Reduce learning rate to 0.0001 for more stable training.\n",
        "\n",
        "Increase weight decay to 0.05 to apply stronger regularization.\n",
        "\n",
        "Use smaller batch sizes to introduce more gradient noise.\n",
        "\n",
        "Limit the number of epochs to num_epochs = 3 to avoid over-training.\n",
        "\n",
        "Increase training data size, ideally to at least 2000 examples, to improve generalization.\n",
        "\n",
        "Applying these changes can help the model learn better representations and avoid overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rl3JQisXFmqA",
        "outputId": "169b6483-12d5-4b9f-a37d-e82317c815a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    unsupervised: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcOcPgFjFPdc",
        "outputId": "478ebf9e-6876-4544-d8bf-3f24eb8936e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 2000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 2000\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "train_small = dataset[\"train\"].shuffle(seed=42).select(range(2000))\n",
        "test_small = dataset[\"test\"].shuffle(seed=42).select(range(2000))\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    \"train\":train_small,\n",
        "    \"test\":test_small\n",
        "})\n",
        "\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eod09ufbF0wO",
        "outputId": "bcb309c0-852d-45f1-8dd6-46fa52f15197"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.5)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.array(dataset['train']['label']).sum()/len(dataset['train']['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0uvVWilF-8a"
      },
      "outputs": [],
      "source": [
        "#create tokenize function\n",
        "def tokenize_function(examples):\n",
        "  #extact text\n",
        "  text = examples[\"text\"]\n",
        "\n",
        "  #tokenize and truncate text\n",
        "  tokenizer.truncation_side = \"left\"\n",
        "  tokenized_inputs = tokenizer(\n",
        "      text,\n",
        "      return_tensors = \"np\",\n",
        "      truncation = True,\n",
        "      max_length=512\n",
        "  )\n",
        "\n",
        "  return tokenized_inputs\n",
        "\n",
        "#add pad token if none exists\n",
        "if tokenizer.pad_token is None:\n",
        "  tokenizer.add_special_tokens({'pad_token':'[PAD]'})\n",
        "  model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "#tokenize training and validation datasets\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "_GKLRT1BGGeR"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "\n",
        "def compute_metrics(p):\n",
        "  predictions, labels = p\n",
        "  predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "  return {\"accuracy\": accuracy.compute(predictions = predictions, references= labels)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Qi9mPsehDcIO"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    task_type = \"SEQ_CLS\", # Indicates it's a sequence classification task.\n",
        "    r = 4, #The rank of the LoRA decomposition (lower rank = fewer trainable params).\n",
        "    lora_alpha = 16, #Scaling factor that controls the learning rate within LoRA layers.\n",
        "    lora_dropout = 0.05, #Dropout applied to the LoRA layers to prevent overfitting.\n",
        "    target_modules = ['q_lin'] #Specifies which layers in the model to apply LoRA to (e.g., the query linear layer in attention).\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "oR3u6ZatEMmL"
      },
      "outputs": [],
      "source": [
        "#hyper parameters\n",
        "lr = 1e-4 #size of optimization step #.001\n",
        "batch_size = 8\n",
        "num_epochs = 2\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir= \"distilbert-finetuned2\",\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=num_epochs,\n",
        "    weight_decay=0.02,\n",
        "    #eval_strategy=\"epoch\",\n",
        "    #save_strategy=\"epoch\",\n",
        "\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=25,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    eval_steps=25,\n",
        "    save_steps=25,\n",
        "    load_best_model_at_end=True,\n",
        "    # Fix 1: Disable Weights & Biases (wandb)\n",
        "    report_to=\"none\",\n",
        "\n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 777
        },
        "id": "uXQpuGO6GmYl",
        "outputId": "0572cbfa-d3ec-462e-a434-23afbc7eb0c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 10:19, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.308000</td>\n",
              "      <td>0.369259</td>\n",
              "      <td>{'accuracy': 0.876}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.286800</td>\n",
              "      <td>0.360298</td>\n",
              "      <td>{'accuracy': 0.8745}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.268900</td>\n",
              "      <td>0.355395</td>\n",
              "      <td>{'accuracy': 0.874}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.196100</td>\n",
              "      <td>0.356086</td>\n",
              "      <td>{'accuracy': 0.8755}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.261400</td>\n",
              "      <td>0.353932</td>\n",
              "      <td>{'accuracy': 0.877}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.292300</td>\n",
              "      <td>0.350319</td>\n",
              "      <td>{'accuracy': 0.877}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.294700</td>\n",
              "      <td>0.345196</td>\n",
              "      <td>{'accuracy': 0.8765}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.372800</td>\n",
              "      <td>0.337940</td>\n",
              "      <td>{'accuracy': 0.877}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.363900</td>\n",
              "      <td>0.331320</td>\n",
              "      <td>{'accuracy': 0.877}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.218000</td>\n",
              "      <td>0.331005</td>\n",
              "      <td>{'accuracy': 0.878}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.233400</td>\n",
              "      <td>0.331320</td>\n",
              "      <td>{'accuracy': 0.8765}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.315300</td>\n",
              "      <td>0.328688</td>\n",
              "      <td>{'accuracy': 0.8785}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>0.415500</td>\n",
              "      <td>0.323570</td>\n",
              "      <td>{'accuracy': 0.878}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.319100</td>\n",
              "      <td>0.323344</td>\n",
              "      <td>{'accuracy': 0.8775}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>0.158200</td>\n",
              "      <td>0.324470</td>\n",
              "      <td>{'accuracy': 0.8765}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.274000</td>\n",
              "      <td>0.327358</td>\n",
              "      <td>{'accuracy': 0.874}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>425</td>\n",
              "      <td>0.257300</td>\n",
              "      <td>0.326013</td>\n",
              "      <td>{'accuracy': 0.875}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.258700</td>\n",
              "      <td>0.325268</td>\n",
              "      <td>{'accuracy': 0.877}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>475</td>\n",
              "      <td>0.195300</td>\n",
              "      <td>0.324259</td>\n",
              "      <td>{'accuracy': 0.8765}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.265600</td>\n",
              "      <td>0.324196</td>\n",
              "      <td>{'accuracy': 0.8765}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=500, training_loss=0.2777618408203125, metrics={'train_runtime': 619.2031, 'train_samples_per_second': 6.46, 'train_steps_per_second': 0.807, 'total_flos': 497497171889664.0, 'train_loss': 0.2777618408203125, 'epoch': 2.0})"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# creater trainer object\n",
        "trainer = Trainer(\n",
        "    model=model, # our peft model\n",
        "    args=training_args, # hyperparameters\n",
        "    train_dataset=tokenized_dataset[\"train\"], # training data\n",
        "    eval_dataset=tokenized_dataset[\"test\"], # validation data\n",
        "    tokenizer=tokenizer, # define tokenizer\n",
        "    data_collator=data_collator, # this will dynamically pad examples in each batch to be equal length\n",
        "    compute_metrics=compute_metrics, # evaluates model using compute_metrics() function from before\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "y5Lf3y24HJiS"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"final_finetuned_tuned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQkcUu4wHNC2",
        "outputId": "61c09ad3-7677-4dd9-8ae7-061a87fdc8e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "\n",
        "id2label = {0: \"Negative\", 1: \"Positive\"}\n",
        "label2id = {\"Negative\": 0, \"Positive\": 1}\n",
        "\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "6BukcTinRzOJ"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "model = PeftModel.from_pretrained(base_model, \"final_finetuned_tuned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWDSw7-bSBPQ",
        "outputId": "d017ce8b-6f53-4439-f8c8-2d79d40e357d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device used is --- cuda\n",
            "This movie was an absolute masterpiece. The storytelling and acting were top-notch! - Positive\n",
            "I wasted two hours of my life. Poor plot, bad acting, and even worse dialogue. - Negative\n",
            "A visually stunning film with a powerful emotional core. I was completely immersed. - Positive\n",
            "The pacing was painfully slow and the plot went nowhere. Very disappointing. - Negative\n",
            "Fantastic performances and a beautiful soundtrack. Highly recommend! - Positive\n",
            "One of the worst movies I\u2019ve seen this year. Not even worth watching for free. - Negative\n",
            "An emotional rollercoaster. It made me laugh, cry, and everything in between. - Positive\n",
            "Predictable and clich\u00e9. Felt like I had seen this exact movie ten times before. - Negative\n",
            "Brilliant direction and an unforgettable story. This one stays with you. - Positive\n",
            "Terrible editing and a lack of character development ruined what could have been decent. - Negative\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "print(\"Device used is ---\", device)\n",
        "\n",
        "\n",
        "movie_reviews = [\n",
        "    \"This movie was an absolute masterpiece. The storytelling and acting were top-notch!\",\n",
        "    \"I wasted two hours of my life. Poor plot, bad acting, and even worse dialogue.\",\n",
        "    \"A visually stunning film with a powerful emotional core. I was completely immersed.\",\n",
        "    \"The pacing was painfully slow and the plot went nowhere. Very disappointing.\",\n",
        "    \"Fantastic performances and a beautiful soundtrack. Highly recommend!\",\n",
        "    \"One of the worst movies I\u2019ve seen this year. Not even worth watching for free.\",\n",
        "    \"An emotional rollercoaster. It made me laugh, cry, and everything in between.\",\n",
        "    \"Predictable and clich\u00e9. Felt like I had seen this exact movie ten times before.\",\n",
        "    \"Brilliant direction and an unforgettable story. This one stays with you.\",\n",
        "    \"Terrible editing and a lack of character development ruined what could have been decent.\"\n",
        "]\n",
        "\n",
        "\n",
        "for text in movie_reviews:\n",
        "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(device) # moving to mps for Mac (can alternatively do 'cpu')\n",
        "\n",
        "    logits = model(inputs).logits\n",
        "    predictions = torch.max(logits,1).indices\n",
        "\n",
        "    print(text + \" - \" + id2label[predictions.tolist()[0]])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNvfuomePLkaO5RzTROWGZU",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}